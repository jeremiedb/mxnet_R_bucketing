---
title: "Time-Series"
output: github_document
editor_options: 
  chunk_output_type: console
---

This is a minimalistic demo on how to model time-series with RNN, including training and inference. 

```{r, echo=TRUE, message=F}
library("data.table")
library("ggplot2")
library("mxnet")
```

### Data preparation

In this step, a dataset made of multiple univariate time-series is built. 
500 independent times series (_n_) with each 100 observations (_seq_len_) are created. 

The modeling task will be to predict the next value given the previous ones. The target variable can therefore be obtained by shifting the features by one timestep in the future. 

The resulting data is of shape [1, 100, 500] corresponding to [num_features, seq_length, samples]. In multivariate time-series, the dataset would follow the same structure, except that the num_features would be > 1. 

```{r, echo=TRUE}
download.file(url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv", destfile = "data/pollution.csv")

mx.set.seed(1234)
## Preprocessing steps

Data <- fread(file = "data/pollution.csv")

## Extracting specific features from the dataset as variables for time series
## We extract pollution, temperature, pressue, windspeed, snowfall and rainfall information from dataset

df <- data.frame(Data$pm2.5, Data$DEWP,Data$TEMP, Data$PRES, Data$Iws, Data$Is, Data$Ir)
df[is.na(df)] <- 0

## Now we normalise each of the feature set to a range(0,1)
df <- matrix(as.matrix(df),ncol=ncol(df),dimnames=NULL)
rangenorm <- function(x){(x-min(x))/(max(x)-min(x))}
df <- apply(df,2, rangenorm)
df <- t(df)

```


For using multidimesional data with MXNetR. We need to convert training data to the form (n_dim x seq_len x num_samples) and label should be of the form (seq_len x num_samples) or (1 x num_samples) depending on the LSTM flavour to be used(one-to-one/ many-to-one). Please note that MXNetR currently supports only these two flavours of RNN. We have used n_dim = 7, seq_len = 100 and num_samples = 430.

```{r}

n_dim <- 7
seq_len <- 100
num_samples <- 430

## extract only required data from dataset
trX <- df[, seq_len(seq_len * num_samples) +24]
trY <- df[1, seq_len(seq_len * num_samples) + 25]

## reshape the matrices in the format acceptable by MXNetR RNNs
trainX <- trX
dim(trainX) <- c(n_dim, seq_len, num_samples)

trainY <- trY
dim(trainY) <- c(seq_len, num_samples)

```


### Model architecture

```{r, echo=TRUE, fig.height=2.5}
batch.size <- 32
# take first 300 samples for train - remaining 100 for evaluation
train_ids <- 1:300
eval_ids<- 301:400

## create dataiterators
train.data <- mx.io.arrayiter(data = trainX[,,train_ids, drop = F], label = trainY[, train_ids],
                              batch.size = batch.size, shuffle = TRUE)

eval.data <- mx.io.arrayiter(data = trainX[,,eval_ids, drop = F], label = trainY[, eval_ids],
                              batch.size = batch.size, shuffle = FALSE)

## Create the symbol for RNN
data <- mx.symbol.Variable("data")
label <- mx.symbol.Variable("label")

data <- mx.symbol.swapaxes(data, dim1 = 0, dim2 = 1)
RNN <- mx.symbol.RNN(data = data, 
                     state_size = 50, 
                     num_layers = 1,
                     p = 0.2,
                     state_outputs = T,
                     mode = "lstm")
data <- mx.symbol.swapaxes(data = RNN[[1]], dim1 = 0, dim2 = 1)
decode <- mx.symbol.FullyConnected(data = data, num_hidden = 1, flatten = F)
loss <- mx.symbol.LinearRegressionOutput(data = decode, label = label)

decode$infer.shape(list(data = c(7, 100, 32)))

mx.metric.mse.seq <- mx.metric.custom("MSE", function(label, pred) {
  label = mx.nd.reshape(label, shape = -1)
  pred = mx.nd.reshape(pred, shape = -1)
  res <- mx.nd.mean(mx.nd.square(label-pred))
  return(as.array(res))
})


ctx <- mx.gpu()

initializer <- mx.init.Xavier(rnd_type = "gaussian",
                              factor_type = "avg",
                              magnitude = 3)

# optimizer <- mx.opt.create("adam", learning.rate = 5e-4, 
#                            beta1 = 0.9, beta2 = 0.999, wd = 1e-8,
#                            clip_gradient = 1, rescale.grad = 1)

optimizer <- mx.opt.create("adadelta", rho = 0.9, eps = 1e-5,
                           wd = 1e-6,
                           clip_gradient = 1, rescale.grad = 1/batch.size)

logger <- mx.metric.logger()
epoch.end.callback <- mx.callback.log.train.metric(period = 10, logger = logger)

## train the network
system.time(
  model <- mx.model.buckets(symbol = loss,
                            train.data = train.data,
                            eval.data = eval.data,
                            num.round = 25, ctx = ctx, verbose = TRUE,
                            metric = mx.metric.mse.seq,
                            initializer = initializer, optimizer = optimizer,
                            batch.end.callback = NULL,
                            epoch.end.callback = epoch.end.callback)
)
```


### Fit a LSTM model

```{r, echo=TRUE, eval=TRUE}
pred_length <- 100
predict <- numeric()

data = mx.nd.array(x[, , 1, drop = F])
label = mx.nd.array(y[, 1, drop = F])

pred = mx.nd.array(y[seq_len, 1, drop = F])
real = sin(seeds[1] + pi/12 * (seq_len+1):(seq_len+pred_length))

infer.data <- mx.io.arrayiter(data = data, label = label, 
                              batch.size = 1, shuffle = FALSE)

infer <- mx.infer.rnn.one(infer.data = infer.data, 
                          symbol = symbol,
                          arg.params = model$arg.params, 
                          aux.params = model$aux.params, 
                          input.params = NULL,
                          ctx = ctx)

for (i in 1:pred_length) {
  
  data = mx.nd.reshape(pred, shape = c(1,1,1))
  label = pred
  
  infer.data <- mx.io.arrayiter(data = data, label = label,  
                                batch.size = 1, shuffle = FALSE)
  
  infer <- mx.infer.rnn.one(infer.data = infer.data, 
                            symbol = symbol,
                            ctx = ctx,
                            arg.params = model$arg.params,
                            aux.params = model$aux.params,
                            input.params = list(rnn.state = infer[[2]], 
                                                rnn.state.cell = infer[[3]]))
  
  pred <- infer[[1]]
  predict <- c(predict, as.numeric(as.array(pred)))
  
}
```

### Plot predictions against real values

```{r}
data <- mx.nd.array(x[, , 1, drop = F])
label <- mx.nd.array(y[, 1, drop = F])

plot_ly(x = 1:dim(y)[1], y = y[,1], type = "scatter", mode="lines", name = "hist") %>% 
  add_trace(x = dim(y)[1] + 1:length(real), y = real, type = "scatter", mode="lines", name = "real") %>% 
  add_trace(x = dim(y)[1] + 1:length(predict), y = predict, type = "scatter", mode="lines", name = "pred")
```

