---
title: "GRU - NLP for Classification"
output:
  html_document:
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: true
    code_folding: show
---

> Demo the application of GRU for NLP task with varying length sequences using both padding and bucketing through custom iterator with MXNet R package

Example based on sentiment analysis on the [IMDB data](http://ai.stanford.edu/~amaas/data/sentiment/).

Load some packages

```{r, echo=T, message=F}
require("readr")
require("dplyr")
require("plotly")
require("stringr")
require("stringi")
require("AUC")
require("scales")
require("mxnet")
```


Load utility functions

```{r, echo=T}
source("../mx_io_bucket_iter.R")
source("../rnn_bucket_setup_refactor.R")
source("../rnn_bucket_train.R")
source("../mx_callback_log_early_stop.R")
```


## Prepare the data

The loaded data has been pre-process into lists whose elements are the buckets containing the samples and their associated labels. 

This pre-processing involves 2 scripts:  

  - data_import.R: import IMDB data  
  - data_prep.R: split samples into word vectors and aggregate the buckets of samples and labels into a list


## Visualize model architecture

```{r, echo=TRUE, fig.height=12, fig.width=8}

rnn_graph <- rnn.unroll(seq.len=4, 
                        num.rnn.layer = 1,
                        num.hidden = 10,
                        input.size=255,
                        num.embed=12,
                        num.label=2,
                        dropout=0.5,
                        ignore_label=0,
                        cell.type="gru",
                        config = "seq-to-one")

graph.viz(rnn_graph, type = "graph", direction = "TD", shape = c(4, 25))

```

## Prepare training data

```{r, echo=TRUE}

#####################################################
### Load preprocessed data
corpus_bucketed_train <- readRDS(file = "../data/kaggle_corpus_bucketed_train_left.rds")
corpus_bucketed_eval <- readRDS(file = "../data/kaggle_corpus_bucketed_eval_left.rds")

vocab <- length(corpus_bucketed_train$dic)

### Create iterators
batch_size = 64

X_iter_train <- mx_io_bucket_iter(buckets = corpus_bucketed_train$buckets, batch_size = batch_size, data_mask_element = 0, shuffle = TRUE)

X_iter_eval <- mx_io_bucket_iter(buckets = corpus_bucketed_eval$buckets, batch_size = batch_size, data_mask_element = 0, shuffle = FALSE)

```


## Model training

```{r, echo=TRUE, eval=FALSE}

end.round=50
num.rnn.layer=2
num.embed=10
num.hidden=10
input.size=vocab
num.label=2

devices <- list(mx.cpu())

metric <- mx.metric.accuracy

initializer=mx.init.Xavier(rnd_type = "gaussian", factor_type = "in", magnitude = 2)
dropout=0
verbose=TRUE

optimizer <- mx.opt.create("adadelta", rho=0.90, epsilon=1e-5, wd=1e-5, clip_gradient=1, rescale.grad=1/batch_size)

# optimizer <- mx.opt.create("rmsprop", learning.rate=0.002, gamma1=0.95, gamma2=0.90, wd=1e-5, clip_gradient=1, rescale.grad=1/batch_size)

# optimizer <- mx.opt.create("sgd", learning.rate=0.05, momentum=0.9, wd=1e-5, clip_gradient=1, rescale.grad=1/batch_size)

logger <- mx.metric.logger()
batch.end.callback <- mx.callback.log.train.metric(period = 25)
epoch.end.callback <- mx.callback.log.early.stop(period = 1, logger = logger, early_stop_rounds = 3, maximize = T)

system.time(model_sentiment <- mx.rnn.buckets(train.data =  X_iter_train,
                                              eval.data = X_iter_eval,
                                              cell.type="gru",
                                              begin.round = 1, 
                                              end.round = end.round, 
                                              ctx = devices, 
                                              metric = metric, 
                                              optimizer = optimizer, 
                                              kvstore = "local",
                                              num.rnn.layer = num.rnn.layer,
                                              num.embed=num.embed, 
                                              num.hidden = num.hidden,
                                              num.label=num.label,
                                              input.size=input.size,
                                              initializer=initializer,
                                              dropout=dropout,
                                              config="seq-to-one",
                                              batch.end.callback=batch.end.callback,
                                              epoch.end.callback=epoch.end.callback,
                                              verbose=TRUE))


# train on full data 
corpus_bucketed_train <- readRDS(file = "../data/kaggle_corpus_bucketed_train_tot_left.rds")

X_iter_train <- mx_io_bucket_iter(buckets = corpus_bucketed_train$buckets, batch_size = batch_size, data_mask_element = 0, shuffle = TRUE)

end.round=14
system.time(model_sentiment <- mx.rnn.buckets(train.data =  X_iter_train,
                                              eval.data = NULL,
                                              cell.type="gru",
                                              begin.round = 1, 
                                              end.round = end.round, 
                                              ctx = devices, 
                                              metric = metric, 
                                              optimizer = optimizer, 
                                              kvstore = "local",
                                              num.rnn.layer = num.rnn.layer,
                                              num.embed=num.embed, 
                                              num.hidden = num.hidden,
                                              num.label=num.label,
                                              input.size=input.size,
                                              update.period=1,
                                              initializer=initializer,
                                              dropout=dropout,
                                              config="seq-to-one",
                                              batch.end.callback=batch.end.callback,
                                              epoch.end.callback=epoch.end.callback,
                                              verbose=TRUE))

mx.model.save(model_sentiment, prefix = "../models/model_kaggle_IMDB_GRU_2layers", iteration = 14)

```


## Inference

```{r, echo=TRUE}

#####################################################
### Inference
ctx <- list(mx.cpu())
model_sentiment <- mx.model.load(prefix = "../models/model_kaggle_IMDB_GRU_2layers", iteration = 14)

corpus_bucketed_train <- readRDS(file = "../data/kaggle_corpus_bucketed_train_left.rds")
corpus_bucketed_test<- readRDS(file = "../data/kaggle_corpus_bucketed_test_left.rds")

```


### Inference on train data

```{r, echo=TRUE}

###############################################
### Inference on train
batch_size <- 256

X_iter_train<- mx_io_bucket_iter(buckets = corpus_bucketed_train$buckets, batch_size = batch_size, data_mask_element = 0, shuffle = F)

infer_train <- mx.rnn.infer.buckets(infer_iter = X_iter_train, 
                                    model = model_sentiment,
                                    config="seq-to-one",
                                    ctx = ctx, 
                                    cell.type = "gru",
                                    kvstore="local")

pred <- infer_train$pred[,2]
pred_bin <- apply(infer_train$pred, 1, which.max)-1
label_train <- infer_train$label

acc_train <- sum(pred_bin==label_train)/length(label_train)

roc_train <- roc(predictions = pred, labels = factor(label_train))
auc_train <- auc(roc_train)

```

Accuracy: `r percent(acc_train)`  
AUC: `r signif(auc_train, 4)`


### Inference on test

```{r, echo=TRUE}

###############################################
### Inference on test
batch_size <- 256

X_iter_test <- mx_io_bucket_iter(buckets = corpus_bucketed_test$buckets, batch_size = batch_size, data_mask_element = 0, shuffle = F)

infer_test <- mx.rnn.infer.buckets(infer_iter = X_iter_test, 
                                   model = model_sentiment,
                                   config="seq-to-one",
                                   ctx = ctx,
                                   cell.type = "gru",
                                   kvstore="local")


### get raw predictions - including padding
pred <- infer_test$pred[,2]
pred_bin <- apply(infer_test$pred, 1, which.max)-1

### test IDs
test_ID <- corpus_bucketed_test$ID

### remove padded predictions
bucket_size <- unlist(lapply(corpus_bucketed_test$buckets, function(x) length(x$label)))
pred_per_bucket <- ceiling(bucket_size / batch_size) * batch_size
pading_per_bucket <- pred_per_bucket - bucket_size
pred_per_bucket_cumsum <- c(0, cumsum(pred_per_bucket))

pred_ID <- unlist(sapply(1:(length(pred_per_bucket_cumsum)-1), function(x) seq(from=pred_per_bucket_cumsum[x] + 1, to=pred_per_bucket_cumsum[x+1] - pading_per_bucket[x])))

pred <- pred[pred_ID]
pred_bin <- pred_bin[pred_ID]
pred_df <- data.frame(id=test_ID, sentiment=pred, stringsAsFactors = F)

### submission
submission <- read_csv("../data/kaggle/data/sampleSubmission.csv") %>% select(-sentiment)
submission <- submission %>% left_join(pred_df, by="id")

write_csv(submission, path = "../submit/kaggle_IMDB_GRU_2layers.csv")

```
