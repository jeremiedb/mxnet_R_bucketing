---
title: "LSTM - NLP for Classification"
output:
  html_document:
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: true
    code_folding: show
---

> Demo the application of LSTM for NLP task with varying length sequences using both padding and bucketing through custom iterator with MXNet R package

Example based on sentiment analysis on the [IMDB data](http://ai.stanford.edu/~amaas/data/sentiment/).

Load some packages

```{r, echo=T, message=F}
require("readr")
require("dplyr")
require("plotly")
require("stringr")
require("stringi")
require("AUC")
require("scales")
require("mxnet")
```


Load utility functions

```{r, echo=T}
source("../mx_io_bucket_iter.R")
source("../rnn_bucket_setup.R")
source("../rnn_bucket_train.R")
```


## Prepare the data

The loaded data has been pre-process into lists whose elements are the buckets containing the samples and their associated labels. 

This pre-processing involves 2 scripts:  

  - data_import.R: import IMDB data  
  - data_prep.R: split samples into word vectors and aggregate the buckets of samples and labels into a list


```{r, echo=TRUE}

#####################################################
### Load preprocessed data
corpus_bucketed_train<- readRDS(file = "../data/corpus_bucketed_train_100_200_300_500_800_left.rds")
corpus_bucketed_test<- readRDS(file = "../data/corpus_bucketed_test_100_200_300_500_800_left.rds")

vocab <- length(corpus_bucketed_test$dic)

### Create iterators
batch_size = 64

X_iter_train<- mx_io_bucket_iter(buckets = corpus_bucketed_train$buckets, batch_size = batch_size, data_mask_element = 0, shuffle = TRUE)

X_iter_test<- mx_io_bucket_iter(buckets = corpus_bucketed_test$buckets, batch_size = batch_size, data_mask_element = 0, shuffle = FALSE)

```

## Define model parameters

```{r, echo=TRUE}

num.label=2
num.embed=16
num.hidden=24
update.period = 1

metric<- mx.metric.accuracy

input.size=vocab
initializer=mx.init.Xavier(rnd_type = "gaussian", factor_type = "in", magnitude = 2)
dropout=0.25
verbose=TRUE

```

## Visualize model architecture

```{r, echo=TRUE, fig.height=12, fig.width=8}

seq.len<- 2

rnn_graph<- rnn.unroll(seq.len=seq.len, num.rnn.layer = 1, 
                       num.hidden = num.hidden,
                       input.size=input.size,
                       num.embed=num.embed, 
                       num.label=num.label,
                       dropout=dropout, 
                       ignore_label = 0,
                       cell.type="lstm",
                       config = "seq-to-one")

graph.viz(rnn_graph, type = "graph", direction = "TD")

```


## Model training

```{r, echo=TRUE, eval=FALSE}

devices<- list(mx.cpu())
end.round=16

optimizer<- mx.opt.create("adadelta", rho=0.92, epsilon=1e-6, wd=0.0002, clip_gradient=NULL, rescale.grad=1/batch_size)

num.rnn.layer=2

batch.end.callback<- mx.callback.log.train.metric(period = 10)
epoch.end.callback<- mx.callback.log.train.metric(period = 1)

system.time(model_sentiment_lstm<- mx.rnn.buckets(train.data =  X_iter_train,
                                                  eval.data = X_iter_test,
                                                  begin.round = 1, 
                                                  end.round = end.round, 
                                                  ctx = devices, 
                                                  metric = metric, 
                                                  optimizer = optimizer, 
                                                  kvstore = "local",
                                                  num.rnn.layer = num.rnn.layer,
                                                  num.embed=num.embed, 
                                                  num.hidden = num.hidden,
                                                  num.label=num.label,
                                                  input.size=input.size,
                                                  update.period=1,
                                                  initializer=initializer,
                                                  dropout=dropout,
                                                  config="seq-to-one",
                                                  batch.end.callback=batch.end.callback,
                                                  epoch.end.callback=epoch.end.callback,
                                                  cell.type="lstm",
                                                  verbose=TRUE))

mx.model.save(model_sentiment_lstm, prefix = "../models/model_sentiment_lstm", iteration = 16)

```


## Inference

```{r, echo=TRUE}

#####################################################
### Inference
ctx <- list(mx.cpu())
model_sentiment <- mx.model.load(prefix = "../models/model_sentiment_lstm", iteration = 16)

corpus_bucketed_train <- readRDS(file = "../data/corpus_bucketed_train_100_200_300_500_800_left.rds")
corpus_bucketed_test<- readRDS(file = "../data/corpus_bucketed_test_100_200_300_500_800_left.rds")

```


### Inference on train data

```{r, echo=TRUE}

###############################################
### Inference on train
batch_size <- 64

X_iter_train<- mx_io_bucket_iter(buckets = corpus_bucketed_train$buckets, batch_size = batch_size, data_mask_element = 0, shuffle = F)

infer_train <- mx.rnn.infer.buckets(infer_iter = X_iter_train, 
                                    model = model_sentiment,
                                    config="seq-to-one",
                                    ctx = ctx, 
                                    cell.type = "lstm",
                                    kvstore="local")

pred_train<- apply(infer_train$pred, 1, which.max)-1
label_train<- infer_train$label

acc_train<- sum(pred_train==label_train)/length(label_train)

roc_train<- roc(predictions = infer_train$pred[,2], labels = factor(label_train))
auc_train<- auc(roc_train)

```

Accuracy: `r percent(acc_train)`  
AUC: `r signif(auc_train, 4)`


### Inference on test

```{r, echo=TRUE}

###############################################
### Inference on test
X_iter_test<- mx_io_bucket_iter(buckets = corpus_bucketed_test$buckets, batch_size = batch_size, data_mask_element = 0, shuffle = F)

infer_test <- mx.rnn.infer.buckets(infer_iter = X_iter_test, 
                                   model = model_sentiment,
                                   config="seq-to-one",
                                   ctx = ctx,
                                   kvstore="local")

pred_test<- apply(infer_test$pred, 1, which.max)-1
label_test<- infer_test$label

acc_test<- sum(pred_test==label_test)/length(label_test)

roc_test<- roc(predictions = infer_test$pred[,2], labels = factor(label_test))
auc_test<- auc(roc_test)

```

Accuracy: `r percent(acc_test)`  
AUC: `r signif(auc_test, 4)`

